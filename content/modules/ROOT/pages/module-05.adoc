
= What is overloading my API?
:prewrap!:

A customer reported an issue where the OpenShift API was down and *etcd*, *kube-apiserver* and other control plane pods were in a CrashLoopBackoff with extremely high memory usage.

.The customer provided the following information:
************************************************
The hardware of our 3 control plane nodes has reached end of life and we will be replacing them with new hardware.

OpenShift 4.16.36

I removed the first old master node and the API became inaccessible. I believe our cluster is having issues running with just two masters.
************************************************

[#theapi]
== What is hitting my API?

How do we check who, what and how often something is hitting the API of an OpenShift cluster?

In this lab we are going to explore the usage of the *OpenShift Cluster Debug Tools*, specifically *kubectl-dev_tool* and the *kubectl-dev_tool audit* subcommand.

https://github.com/openshift/cluster-debug-tools/

In addition to the standard must-gather, there are a number of additional variants for collecting data for special use cases.

For this issue, we asked the customer to collect the audit logs from their cluster so we can analyze every request going to the API.

[TIP]
=====
You can use the following command to collect an *audit log* *must-gather*:

*oc adm must-gather -- /usr/bin/gather_audit_logs*

This will produce a *must-gather* style directory that can be zipped and uploaded to a support case.

If the API is not available, you can collect the audit logs manually under */var/log/* using *ssh* and *scp* to the node.

*/var/log/kube-apiserver* and the other directories, if necessary, such as *etcd*, *oauth-apiserver*, *openshift-apiserver* and *oauth-server*.
=====

[#explore]
== Explore the *kubectl-dev_tool audit* command

Let's take a look at the *kubectl-dev_tool audit* subcommand by running it with the *-h* flag to look at the help screen.

On this screen, you will find 5 examples to give you some understanding of how to use the tool.

[TIP]
====
When using the *-f* flag, you can pass a single file, as in the example, or the entire directory.

*-f 'audit_logs/kube-apiserver'*
====

[#firstrun]
== Run a command

Let's run a command against the *kube-apiserver* and see what kind of information we get back.

[.wrap,console,role=execute]
----
cd ~/Module5/module5-must-gather-audit-log/quay-io-openshift-release-dev-ocp-v4-0-art-dev-sha256-c413593a46a302b8fab6ffa3f2f993b0110c2c84ced3dd399cedb4c66c0c99f7/
----

Depending on the amount of data and the complexity of the search, it can take some time to process.

Using the example from the help page, we can run a command like this:

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --verb=get --resource='*.*' --resource='-subjectaccessreviews.*' --resource='-tokenreviews.*' | more
----

In this case, we are looking at individual *GET* requests, how long they took, what they accessed and who made the request.

[source,bash]
----
23:03:59 [   GET][     1.095ms] [200] /livez                                  [system:anonymous]
23:04:00 [   GET][       352µs] [200] /.well-known/oauth-authorization-server [system:anonymous]
23:04:00 [   GET][       196µs] [200] /.well-known/oauth-authorization-server [system:anonymous]
----

[NOTE]
====
The larger and busier the cluster, the shorter the audit log duration will be.

If you have a requirement to maintain or potentially review the audit logs, you need to send them to an external logging source for retention.

For this lab, each *kubectl-dev_tool* commands takes approximately 50 seconds to run.
====

[#theissue]
== Checking for issues

Now that we know a little bit about the *kubectl-dev_tool*, let's look at our *audit log must-gather* for potential issues.

The first thing you want to do is narrow down and define a time range for the issue. Being as specific as possible will help ensure your results are accurate. As an example, 100,000 requests against a certain API over a 6 hour window may not be an issue. If those 100,000 requests actually occur during a 10 minute period of within those 6 hours of logs, when the issue occurs, it is likely worth looking into.

Find the newest audit log timestamp looking at the newest *audit.log* file.
[.wrap,console,role=execute]
----
tail -n 1 ./audit_logs/kube-apiserver/cluster-master-0[1-3].dmz-audit.log
----

Find the oldest audit log timestamp looking at the oldest *audit.log* file.
[source,bash]
----
cluster-master-01.dmz-audit-2025-09-08T14-20-37.493.log
cluster-master-02.dmz-audit-2025-09-08T14-46-08.059.log
cluster-master-03.dmz-audit-2025-09-08T14-15-11.014.log
----

For our case, our timerange was: *2025-09-08T14:15:00Z to 2025-09-08T15:34:57Z*

[TIP]
====
There is a helpful flag you can use *-o top* which will count, group and display the top 10 entries for your search.
====

Start by taking a high level view. You can be both broad and granular with audit logs, but unless you know exactly what you're looking for, it's best to cast a wide net.

Look at the top usage for the common *--by=* groups like *resource* and *user*. We can combine that using *--after* and *--before*, to narrow the results to focus in on our timerange.


[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --by=resource -otop --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

The first thing you will see is a log outlining the *count* (number of records), followed by a time range and duration for the period the audit logs cover.

[source,bash]
----
count: 4714125, first: 2025-09-08T10:15:00-04:00, last: 2025-09-08T11:34:56-04:00, duration: 1h19m56.999222s
----

After that, you will see the returned data set.

[source,bash]
----
487532x        v1/endpoints
442694x        v1/configmaps
384736x        authorization.k8s.io/v1/subjectaccessreviews
362910x        v1/persistentvolumeclaims
338418x        v1/persistentvolumes
328024x        v1/secrets
320234x        discovery.k8s.io/endpointslices
279757x        v1/pods
202521x        authentication.k8s.io/v1/selfsubjectreviews
195895x        metrics.k8s.io/pods
----

Similarly, you can run the same command, using *--by=user*.

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --by=user -otop --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

[source,bash]
----
506725x        system:serviceaccount:kube-system:horizontal-pod-autoscaler
298083x        system:serviceaccount:kube-system:endpointslicemirroring-controller
268994x        system:serviceaccount:openshift-apiserver:openshift-apiserver-sa
184530x        system:serviceaccount:openshift-example-crunchy:pgo[postgrescluster-controller]
158628x        system:serviceaccount:kyverno:kyverno-reports-controller
105195x        system:serviceaccount:openshift-monitoring:metrics-server
89860x         system:serviceaccount:d6af69-test:ha-postgres-crunchydb-instance
87200x         system:serviceaccount:db4642-dev:pay-transparency-dev-crunchy-instance
82254x         system:serviceaccount:e52f12-dev:pg-272ed005-crunchy-instance
81997x         system:serviceaccount:d6af69-dev:ha-postgres-crunchydb-instance
----

[#thedata]
== Evaluate and dig deeper

Based on the above data, there is nothing glaringly obvious that is wrong with the cluster, but we can see that the API received *4,714,125* requests over an *80* minute period. That translates to almost *1000* API requests per second.

Let's dig a little deeper.

[TIP]
====
When evaluating the data, always factor in things like the total number of requests, time period and the number of nodes.
====

Our top resources from the previous command were *endpoints*, *configmaps*, *subjectaccessreviews*, *persistentvolumeclaims*, *persistentvolumes* and *secrets*:

[source,bash]
----
487532x        v1/endpoints
442694x        v1/configmaps
384736x        authorization.k8s.io/v1/subjectaccessreviews
362910x        v1/persistentvolumeclaims
338418x        v1/persistentvolumes
328024x        v1/secrets
----

Our top users from the previous command were *horizontal-pod-autoscaler*, *endpointslicemirroring-controller*, *openshift-apiserver-sa*, *pgo[postgrescluster-controller]* and *kyverno-reports-controller*.

[source,bash]
----
506725x        system:serviceaccount:kube-system:horizontal-pod-autoscaler
298083x        system:serviceaccount:kube-system:endpointslicemirroring-controller
268994x        system:serviceaccount:openshift-apiserver:openshift-apiserver-sa
184530x        system:serviceaccount:openshift-example-crunchy:pgo[postgrescluster-controller]
158628x        system:serviceaccount:kyverno:kyverno-reports-controller
----

Again, none of those stick out as glaring issues, but let's dig into the top resources. For this we can use the *--resource=* flag, in addition to *--by=* and *-o top* to down on a specific resource.

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --resource=endpoints -otop --by=user --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

We can see the *endpoint* activity is spread out, but you can also see the top consumers are *crunchy* / *postgres* instances from different namespaces.

[source,bash]
----
count: 487532, first: 2025-09-08T10:15:00-04:00, last: 2025-09-08T11:34:56-04:00, duration: 1h19m56.967674s
89645x         system:serviceaccount:d6af69-test:ha-postgres-crunchydb-instance
87151x         system:serviceaccount:db4642-dev:pay-transparency-dev-crunchy-instance
82088x         system:serviceaccount:e52f12-dev:pg-272ed005-crunchy-instance
81963x         system:serviceaccount:d6af69-dev:ha-postgres-crunchydb-instance
38591x         system:serviceaccount:c57ee7-dev:dev-crunchy-postgres-instance
18316x         system:serviceaccount:kube-system:endpoint-controller
4689x          system:serviceaccount:openshift-example-crunchy:pgo[postgrescluster-controller]
1344x          system:serviceaccount:openshift-example-integration:strimzi-cluster-operator
1309x          system:apiserver
1148x          system:serviceaccount:openshift-monitoring:prometheus-k8s
----

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --resource=configmaps --by=user -otop --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

We can see the *configmap* activity is spread out.

[source,bash]
----
count: 442694, first: 2025-09-08T10:15:00-04:00, last: 2025-09-08T11:34:56-04:00, duration: 1h19m56.999222s
40623x         system:serviceaccount:kyverno:kyverno-reports-controller
25472x         system:serviceaccount:openshift-example-crunchy:pgo[postgrescluster-controller]
6470x          system:node:cluster-app-109.dmz
5870x          system:node:cluster-app-104.dmz
5513x          system:node:cluster-app-40.dmz
5453x          system:node:cluster-app-103.dmz
5423x          system:node:cluster-app-101.dmz
5407x          system:node:cluster-app-87.dmz
5396x          system:node:cluster-app-108.dmz
5389x          system:node:cluster-app-102.dmz
----

[NOTE]
====
The parsing of *subjectaccessreviews* is very slow and does not yield anything useful in this case, so feel free to skip it.

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --resource='subjectaccessreviews.*' -otop --by=user --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----
====

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --resource='persistentvolumeclaims' -otop --by=user --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

We can see a high number of *persistentvolumeclaims* activity across all nodes in the cluster, as well as from *crunchy*.

[source,bash]
----
count: 362910, first: 2025-09-08T10:15:00-04:00, last: 2025-09-08T11:34:56-04:00, duration: 1h19m56.992954s
16115x         system:serviceaccount:openshift-example-crunchy:pgo[postgrescluster-controller]
13138x         system:node:cluster-app-109.dmz
11969x         system:node:cluster-app-104.dmz
11681x         system:node:cluster-app-89.dmz
11439x         system:node:cluster-app-87.dmz
11247x         system:node:cluster-app-108.dmz
10692x         system:node:cluster-app-94.dmz
10191x         system:node:cluster-app-84.dmz
10187x         system:node:cluster-app-92.dmz
10028x         system:node:cluster-app-101.dmz
----

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --resource='persistentvolumes' -otop --by=user --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

We can see a high number of *persistentvolumes* activity across all nodes in the cluster.

[source,bash]
----
count: 338418, first: 2025-09-08T10:15:00-04:00, last: 2025-09-08T11:34:56-04:00, duration: 1h19m56.910381s
13109x               system:node:cluster-app-109.dmz
11952x               system:node:cluster-app-104.dmz
11675x               system:node:cluster-app-89.dmz
11429x               system:node:cluster-app-87.dmz
11236x               system:node:cluster-app-108.dmz
10677x               system:node:cluster-app-94.dmz
10190x               system:node:cluster-app-84.dmz
10185x               system:node:cluster-app-92.dmz
10023x               system:node:cluster-app-101.dmz
9551x                system:node:cluster-app-86.dmz
----

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --resource='secrets' -otop --by=user --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

We can see the *secrets* activity is spread out.

[source,bash]
----
count: 328024, first: 2025-09-08T10:15:00-04:00, last: 2025-09-08T11:34:56-04:00, duration: 1h19m56.89568s
78016x         system:serviceaccount:openshift-infra:serviceaccount-pull-secrets-controller[openshift.io/image-registry-pull-secrets_legacy-token-secrets-controller]
46030x         system:serviceaccount:openshift-example-crunchy:pgo[postgrescluster-controller]
12091x         system:serviceaccount:openshift-authentication-operator:authentication-operator
10523x         system:serviceaccount:openshift-apiserver-operator:openshift-apiserver-operator
5428x          system:node:cluster-app-109.dmz
5376x          system:serviceaccount:openshift-example-integration:strimzi-cluster-operator
4701x          system:serviceaccount:openshift-example-crunchy:pgo
4567x          system:serviceaccount:e69aae-test:postgresql-operator-manager
4358x          system:node:cluster-app-104.dmz
4350x          system:node:cluster-app-103.dmz
----

Evaluating the output, the most common theme across all of the data points has been activity from *crunchy* / *postgress*. If you refer back to the original *--by=user* output, there was no glaring single user, but 5 of the top 10 users driving API load were *crunchy* / *postgress* related. If we add them all up, a conservative estimate, without secondary and follow-on factors, is 621,383 of 4,714,125 requests, totalling 13.1% of API traffic.

Given that *crunchy* / *postgress* are databases, we can also look into the high level of activity taking place against *PersistentVolumes* and *PersistentVolumeClaims*.

Let's take a look at one of the users and see what they are doing. You can do this by passing in the *--user=* flag along with *--by=verb* and *-o top*.

Let's try to answer the following:

. *What is the user doing?* +
. *What is the problem?* +

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --user='system:serviceaccount:d6af69-test:ha-postgres-crunchydb-instance' --by=verb -otop --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

What we see is interesting:

. The majority of activity is from *PATCH* commands on endpoints
. Requests are mostly successful with HTTP 200 errors

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --user='system:node:cluster-app-109.dmz' --by=verb -otop --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----

What we see is:

. The majority of activity is from *GET* commands on *PersistentVolumes* and *PersistentVolumeClaims* across dozens of different *Namespaces*.

[NOTE]
====
This command takes a long time to run

[.wrap,console,role=execute]
----
kubectl-dev_tool audit -f 'audit_logs/kube-apiserver' --user='system:serviceaccount:openshift-example-crunchy:pgo*' --by=verb -otop --after 2025-09-08T14:15:00Z --before 2025-09-08T15:34:57Z
----
====

What we see is:

. The majority of activity is from *PATCH* and *LIST* commands across dozens of different namespaces

Based on this information, we could make a few informed deductions.

1. Even though the API pool members are marked as down in the F5, millions of requests are still making it API server.
2. We see 13% of our API traffic coming from *crunchy* / *postgress* related pods.
3. We see another 14.9% of our API traffic coming from *PersistentVolumes* and *PersistentVolumeClaims*.

The conclusion was that *Crunchy Postgres Operator* was under significant stress due to the API flapping. This lead to an unexpected influx of traffic trying to *GET*, *LIST* and *UPDATE* its owned resources.

With this information, we were able to have an informed discussion with the customer and they were able to take the data to the 3rd party vendor and get the issue addressed to avoid such an API storm in the future.

*How did we recover the cluster?*

With no way of shutting down workloads or disabling the *Operator*, we had to get creative to stabilize the *Control Plane*.

We applied 2 *iptables* rules on all of the *worker* nodes in the cluster to drop traffic to port 6443 on both the *FORWARD* and *OUTPUT* chains.

[source,bash]
----
iptables -I FORWARD -p tcp --dport 6443 -j DROP
iptables -I OUTPUT -p tcp --dport 6443 -j DROP
----

This blocked both the *kubelet* and running *Pods* from sending traffic to the API and subsequently allowed for the API to stabalize and recover.

Once the API was healthy, we removed the *iptables* rules in blocks of 5 nodes, allowing them to come back and workloads to fully recover.

I hope you found this introduction to the *kubectl-dev_tool* useful and can leverage it the next time you have an issue!

[TIP]
====
You don't need to have an overloaded API or a performance issue to take a look at the audit logs.

The audit logs and *kubectl-dev_tool* are equally useful if you want to understand who or what did something in your cluster.

Something or someone deleted your pod, secret, service or other object? That's in the audit logs! Use the *kubectl-dev_tool* to find out who did it and when!
====
