= OVN-Kubernetes - My pods can not communicate in the same namespace
:prewrap!:

[#theissue]
== Issues with pod communication in a namespace

I have multiple pods running in the same namespace, but they can not communicate with eachother.

When `sitea-frontend-8798d9cdc-sq7dk` or `siteb-frontend-66f9f55ddc-97jkg` in the `frontend` namespace try to reach eachother, communication just hangs.

[#debugnspodtopod]
== Debug the Traffic Flow

To trace the path with `ovnkube-trace`, simply pass in the `-src-namespace`, `-src` pod name, `-dst-namespace` and `-dst` pod name:

[TIP]
=====
[source,bash]
----
./ovnkube-trace -tcp -src-namespace frontend -src sitea-frontend-8798d9cdc-sq7dk -dst-namespace frontend -dst siteb-frontend-66f9f55ddc-97jkg --loglevel=4
----
=====

What does `ovnkube-trace` do and how is it going to help us determine why our pods can not communicate?

It is a wrapper tool that takes the CLI inputs, looks at your cluster, gets all of the IP and MAC information and builds an `ovn-trace` command to actually trace the trafic flow.

[#firsttrace]
=== Running the First Trace

From your execution environment, run the following `ovn-trace`.

As you can see, the `ovn-trace` is far more complex than then `ovnkube-trace`:

. `worker-5.ocpv.tamlab.rdu2.redhat.com` -> is the OVN datapath which is a logical router or switch associated to the workloads
. `inport` -> is the OVN port for instance we are tracing from
. `eth.src` -> source pod mac address
. `eth.dst` -> destination pod mac address
. `ip4.src` -> source pod IP address
. `ip4.dst` -> destination pod IP address
. `tcp.dst` -> destination port
. `tcp.src` -> source port

[source,console,role=execute]
----
ovn-trace --no-leader-only  --db unix:/var/run/ovn/ovnsb_db.sock worker-5.ocpv.tamlab.rdu2.redhat.com 'inport=="frontend_sitea-frontend-8798d9cdc-sq7dk" && eth.src==0a:58:0a:81:06:11 && eth.dst==0a:58:0a:81:06:01 && ip4.src==10.129.6.17 && ip4.dst==10.129.6.15 && ip.ttl==64 && tcp.dst==80 && tcp.src==52888'
----

When the `ovn-trace` runs, you get a full accounting of how that packet would traverse the SDN. Because we don't need to analyze the whole thing, we're going to focus on the last `egress` section.

[source,console]
----
egress(dp="worker-5.ocpv.tamlab.rdu2.redhat.com", inport="stor-worker-5.ocpv.tamlab.rdu2.redhat.com", outport="frontend_siteb-frontend-66f9f55ddc-97jkg")
---------------------------------------------------------------------------------------------------------------------------------------------------------
 2. ls_out_pre_acl (northd.c:6057): ip, priority 100, uuid c1ee92b5
    reg0[0] = 1;
    next;
 3. ls_out_pre_lb (northd.c:6267): ip, priority 100, uuid 2f4aa83e
    reg0[2] = 1;
    next;
 4. ls_out_pre_stateful (northd.c:6298): reg0[2] == 1, priority 110, uuid bcf2d2d5
    ct_lb_mark;

ct_lb_mark /* default (use --ct to customize) */
------------------------------------------------
 5. ls_out_acl_hint (northd.c:6399): !ct.new && ct.est && !ct.rpl && ct_mark.blocked == 0, priority 4, uuid cfae0de0
    reg0[8] = 1;
    reg0[10] = 1;
    next;
 8. ls_out_acl_action (northd.c:7308): reg8[30..31] == 0, priority 500, uuid 9ba0e4eb
    reg8[30..31] = 1;
    next(6);
 8. ls_out_acl_action (northd.c:7308): reg8[30..31] == 1, priority 500, uuid a72c3c38
    reg8[30..31] = 2;
    next(6);
 6. ls_out_acl_eval (northd.c:7137): reg8[30..31] == 2 && reg0[10] == 1 && (outport == @a3640229366109469754), priority 2000, uuid f5747c49
    reg8[17] = 1;
    ct_commit { ct_mark.blocked = 1; ct_label.obs_point_id = 0; };
    next;
 8. ls_out_acl_action (northd.c:7281): reg8[17] == 1, priority 1000, uuid b201ae3a
    reg8[16] = 0;
    reg8[17] = 0;
    reg8[18] = 0;
    reg8[30..31] = 0;
----

[#analyzefirsttrace]
=== Analyzing the Trace

What we see is an abrupt end with no clear indication of sucess or failure.

Focusing in, you can see that during `ls_out_acl_eval` we commit `ct_mark.blocked = 1`.

When `ct_mark.blocked` is set to 1, it means that the connection is no longer allowed by the policy and anything in reply will be dropped.

[source,console]
----
 6. ls_out_acl_eval (northd.c:7137): reg8[30..31] == 2 && reg0[10] == 1 && (outport == @a3640229366109469754), priority 2000, uuid f5747c49
    reg8[17] = 1;
    ct_commit { ct_mark.blocked = 1; ct_label.obs_point_id = 0; };
    next;
----

So how do we determine why the connection was not allowed? Let's inspect the database and find out.

First, run the following command which will print a brief overview of the database contents.

[source,console,role=execute]
----
ovn-nbctl show
----

Next, run the following 2 commands to look at the list of `logical router (lr)` and `logical switch (ls)` objects in the database.

[source,console,role=execute]
----
ovn-nbctl lr-list
----

Logical Router objects, defined by a UUID and name.

[source,console]
----
9316827c-90ed-4943-bb63-1480ffbcffac (GR_worker-5.ocpv.tamlab.rdu2.redhat.com)
07fef835-dcce-4db0-93df-5515f3dd0f49 (ovn_cluster_router)
----

[source,console,role=execute]
----
ovn-nbctl ls-list
----

Logical Switch objects, defined by a UUID and name.

[source,console]
----
cc63f8ab-f7ff-4906-a15b-fb7b24c62eb2 (br.ex.localnet_ovn_localnet_switch)
1524ecbe-0704-4e9f-b97d-9a09e984ef02 (cluster_udn_drenard.udn.tamlab_ovn_layer2_switch)
4168b09e-0580-4d54-9ef2-3c5dc361a0bf (ext_worker-5.ocpv.tamlab.rdu2.redhat.com)
26008754-5629-419b-87cb-bf9fbbf9ff7f (join)
57b99e34-e732-4ce1-8244-eb507ee02ffe (ovs.bridge.vlan530_ovn_localnet_switch)
8763ad07-147d-4f38-b87f-41b5c152a7bb (transit_switch)
16ccd735-09f4-41c1-a546-2df09f22c2f7 (vlan.636.localnet_ovn_localnet_switch)
217a444d-ef97-4d51-89d8-581d673e3cf0 (worker-5.ocpv.tamlab.rdu2.redhat.com)
----

From the output, you will notice one of the logical switches, `worker-5.ocpv.tamlab.rdu2.redhat.com` is the switch we used in our `ovn-trace`. So lets take a closer look at that switch by looking at the attached `logical switch ports (lsp)`.

Because a cluster is going to have a lot of ports, we focus in with a `grep` on our specifc pods. If you are interested, feel free to run the command again without the grep to see the full list.

[source,console,role=execute]
----
ovn-nbctl lsp-list worker-5.ocpv.tamlab.rdu2.redhat.com | grep 'siteb\|sitea\|network-policy-demo-monitor\|external'
----

Logical Switch Port objects, defined by UUID and name, where the name aligns to the running pod within the cluster.

[source,console]
----
b684138e-68b5-4249-9403-677700053205 (backend_sitea-backend-796cf44b87-wx565)
296ccbb5-ea7c-4714-9669-031342907ae9 (backend_siteb-backend-5b765679b5-6wkvx)
bc5892a3-a5a5-4f45-bbc3-b6dea0853403 (data_virt-launcher-sitea-data-6jv6x)
a4830687-355d-4239-8fb5-f5b990ad23e2 (external_virt-launcher-external-vm-tfbr4)
c8977c7e-e9a7-4548-9383-12ea8dc6df88 (frontend_sitea-frontend-8798d9cdc-sq7dk)
813df718-f961-4a2f-9606-97125b43c38a (frontend_siteb-frontend-66f9f55ddc-97jkg)
cc35c8dd-4999-497e-a3a8-58775aff23ae (network-policy-demo-monitor_monitor-956f5b8d7-xqjmx)
----

Looking back at our trace, we see traffic is going to `outport == @a3640229366109469754` before it is marked as being dropped.

These outports correspond to portgroups and are created for each `NetworkPolicy` in your cluster. All pods selected by the `podSelector` of that `NetworkPolicy` are added to the portgroupâ€™s port list.

Let's take a look at our promlematic `portgroup`.

[source,console,role=execute]
----
ovn-nbctl find Port_Group name="a3640229366109469754"
----

[source,console]
----
_uuid               : 5a0d0fe2-bece-44df-9990-66f323e26f19
acls                : [0cbe6f52-9cf1-4584-9237-7c23f65c82f1, 1fdd654d-f251-4e58-b4ce-d57804f1ba9b]
external_ids        : {direction=Ingress, "k8s.ovn.org/id"="default-network-controller:NetpolNamespace:frontend:Ingress", "k8s.ovn.org/name"=frontend, "k8s.ovn.org/owner-controller"=default-network-controller, "k8s.ovn.org/owner-type"=NetpolNamespace}
name                : a3640229366109469754
ports               : [813df718-f961-4a2f-9606-97125b43c38a, c8977c7e-e9a7-4548-9383-12ea8dc6df88]
----

From the output there are 2 pieces of important information:

1. `acls` -> there are 2 `UUIDs` -> `[0cbe6f52-9cf1-4584-9237-7c23f65c82f1, 1fdd654d-f251-4e58-b4ce-d57804f1ba9b]`
2. `ports` -> there are 2 `UUIDs` -> `[813df718-f961-4a2f-9606-97125b43c38a, c8977c7e-e9a7-4548-9383-12ea8dc6df88]`

What you will notice is that the 2 `ports` `UUIDs` correspond to the `UUIDs` of the `logical switch port` entries for the 2 `frontend` pods from the `lsp-list` output above. This confirms that the portgroup is influencing our 2 pods.

Next, lets take a look at the 2 `access control list (ACL)` objects.

[source,console,role=execute]
----
ovn-nbctl list ACL 0cbe6f52-9cf1-4584-9237-7c23f65c82f1
----

[source,console]
----
_uuid               : 0cbe6f52-9cf1-4584-9237-7c23f65c82f1
action              : drop
direction           : to-lport
external_ids        : {direction=Ingress, "k8s.ovn.org/id"="default-network-controller:NetpolNamespace:frontend:Ingress:defaultDeny", "k8s.ovn.org/name"=frontend, "k8s.ovn.org/owner-controller"=default-network-controller, "k8s.ovn.org/owner-type"=NetpolNamespace, type=defaultDeny}
label               : 0
log                 : false
match               : "outport == @a3640229366109469754"
meter               : acl-logging
name                : "NP:frontend:Ingress"
options             : {}
priority            : 1000
sample_est          : []
sample_new          : []
severity            : []
tier                : 2
----

[source,console,role=execute]
----
ovn-nbctl list ACL 1fdd654d-f251-4e58-b4ce-d57804f1ba9b
----

[source,console]
----
_uuid               : 1fdd654d-f251-4e58-b4ce-d57804f1ba9b
action              : allow
direction           : to-lport
external_ids        : {direction=Ingress, "k8s.ovn.org/id"="default-network-controller:NetpolNamespace:frontend:Ingress:arpAllow", "k8s.ovn.org/name"=frontend, "k8s.ovn.org/owner-controller"=default-network-controller, "k8s.ovn.org/owner-type"=NetpolNamespace, type=arpAllow}
label               : 0
log                 : false
match               : "outport == @a3640229366109469754 && (arp || nd)"
meter               : acl-logging
name                : "NP:frontend:Ingress"
options             : {}
priority            : 1001
sample_est          : []
sample_new          : []
severity            : []
tier                : 2
----

You can see that the second ACL, `1fdd654d-f251-4e58-b4ce-d57804f1ba9b`, is matching on `arp || nd`, we can ignore it as we are sending `tcp` requests.

Focusing in on the output from the first ACL `0cbe6f52-9cf1-4584-9237-7c23f65c82f1`, we can see a few key points:

1. `action` -> `drop`
2. `direction` -> `to-lport`
3. `external_ids` -> `frontend:Ingress:defaultDeny`
4. `match` -> `outport == @a3640229366109469754`

This tells us that any traffic that is forwarded to a logical port that matches `outport == @a3640229366109469754` will be dropped due to a `defaultDeny` policy in the `frontend` namespace.

Looking back at our setup, we know that our cluster adds a `default-deny` `NetworkPolicy` to every new `Namespace` and that aligns to the policy above that is impacting our pod to pod communication.

[#resolvingtheissue]
=== Resolving the Connectivity Issue

Now that we know the issue, we can easily fix that with a new `NetworkPolicy` to allow traffic between pods in the namespace.

[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
  namespace: frontend
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}
  policyTypes:
  - Ingress
----

[NOTE]
=====
Because this is a static lab, we can not directly demonstrate the resolution of the issue. So you can see the difference, the output of a successful trace, after applying the above `NetworkPolicy`, is included below for completeness.
=====

Looking at the new trace, focusing in on the same `egress` section, there are 2 key differences.

[source,console]
----
egress(dp="worker-5.ocpv.tamlab.rdu2.redhat.com", inport="stor-worker-5.ocpv.tamlab.rdu2.redhat.com", outport="frontend_siteb-frontend-66f9f55ddc-97jkg")
---------------------------------------------------------------------------------------------------------------------------------------------------------
 2. ls_out_pre_acl (northd.c:6057): ip, priority 100, uuid c1ee92b5
    reg0[0] = 1;
    next;
 3. ls_out_pre_lb (northd.c:6267): ip, priority 100, uuid 2f4aa83e
    reg0[2] = 1;
    next;
 4. ls_out_pre_stateful (northd.c:6298): reg0[2] == 1, priority 110, uuid bcf2d2d5
    ct_lb_mark;

ct_lb_mark /* default (use --ct to customize) */
------------------------------------------------
 5. ls_out_acl_hint (northd.c:6399): !ct.new && ct.est && !ct.rpl && ct_mark.blocked == 0, priority 4, uuid cfae0de0
    reg0[8] = 1;
    reg0[10] = 1;
    next;
 8. ls_out_acl_action (northd.c:7308): reg8[30..31] == 0, priority 500, uuid 9ba0e4eb
    reg8[30..31] = 1;
    next(6);
 8. ls_out_acl_action (northd.c:7308): reg8[30..31] == 1, priority 500, uuid a72c3c38
    reg8[30..31] = 2;
    next(6);
 6. ls_out_acl_eval (northd.c:7089): reg8[30..31] == 2 && reg0[8] == 1 && (ip4.src == {$a16799780576945479038} && outport == @a3002123605838622674), priority 2001, uuid cd99dbd5
    reg8[16] = 1;
    next;
 8. ls_out_acl_action (northd.c:7276): reg8[16] == 1, priority 1000, uuid b2d321e2
    reg8[16] = 0;
    reg8[17] = 0;
    reg8[18] = 0;
    reg8[30..31] = 0;
    next;
11. ls_out_check_port_sec (northd.c:5866): 1, priority 0, uuid a87e1d41
    reg0[15] = check_out_port_sec();
    next;
12. ls_out_apply_port_sec (northd.c:5874): 1, priority 0, uuid 7e9dcb6d
    output;
    /* output to "frontend_siteb-frontend-66f9f55ddc-97jkg", type "" */
----

1. The `ls_out_acl_eval` is different and is no longer dropping the packet

[source,console]
----
ls_out_acl_eval (northd.c:7089): reg8[30..31] == 2 && reg0[8] == 1 && (ip4.src == {$a16799780576945479038} && outport == @a3002123605838622674), priority 2001, uuid cd99dbd5
----

[start=2]
2. We see a successful output to our destination pod -> `/* output to "frontend_siteb-frontend-66f9f55ddc-97jkg", type "" */`

If we take it a step further, we can take a look at our new `ls_out_acl_eval` with `ip4.src == {$a16799780576945479038} && outport == @a3002123605838622674`.

The `ip4.src` is a hash corresponding to an address set of pods:

[source,console]
----
ovn-nbctl list address_set a16799780576945479038
_uuid               : 61d5788a-ec8a-4f8b-ade4-9c9c8d7676a0
addresses           : ["10.129.6.15", "10.129.6.17"]
external_ids        : {ip-family=v4, "k8s.ovn.org/id"="default-network-controller:PodSelector:frontend_LS{}:v4", "k8s.ovn.org/name"="frontend_LS{}", "k8s.ovn.org/owner-controller"=default-network-controller, "k8s.ovn.org/owner-type"=PodSelector}
name                : a16799780576945479038
----

[TIP]
=====
In your execution environment, you can run `ovn-nbctl list address_set` to see a list of all address sets in the cluster.
=====

The `outport`, like above, corresponds to a `portgroup`. You can see that this `portgroup` now comes from our new `NetworkPolicy` -> `NetworkPolicy:frontend:allow-same-namespace`.

[source,console]
----
ovn-nbctl find Port_Group  name="a3002123605838622674"
_uuid               : 0d93d7ed-fe60-4946-9f60-e50534543342
acls                : [72f8671e-5151-4274-a498-89a7e29cddd4]
external_ids        : {"k8s.ovn.org/id"="default-network-controller:NetworkPolicy:frontend:allow-same-namespace", "k8s.ovn.org/name"="frontend:allow-same-namespace", "k8s.ovn.org/owner-controller"=default-network-controller, "k8s.ovn.org/owner-type"=NetworkPolicy}
name                : a3002123605838622674
ports               : [813df718-f961-4a2f-9606-97125b43c38a, c8977c7e-e9a7-4548-9383-12ea8dc6df88]
----

And on that portgroup, a new `ACL` exists. This `ACL` now has `allow-related` for any `to-lport` traffic that matches the `ip4.src` and outport for our pods in the `frontend` `Namespace`.

[source,console]
----
ovn-nbctl list ACL 72f8671e-5151-4274-a498-89a7e29cddd4
_uuid               : 72f8671e-5151-4274-a498-89a7e29cddd4
action              : allow-related
direction           : to-lport
external_ids        : {direction=Ingress, gress-index="0", ip-block-index="-1", "k8s.ovn.org/id"="default-network-controller:NetworkPolicy:frontend:allow-same-namespace:Ingress:0:None:-1", "k8s.ovn.org/name"="frontend:allow-same-namespace", "k8s.ovn.org/owner-controller"=default-network-controller, "k8s.ovn.org/owner-type"=NetworkPolicy, port-policy-protocol=None}
label               : 0
log                 : false
match               : "ip4.src == {$a16799780576945479038} && outport == @a3002123605838622674"
meter               : acl-logging
name                : "NP:frontend:allow-same-namespace:Ingress:0"
options             : {}
priority            : 1001
sample_est          : []
sample_new          : []
severity            : []
tier                : 2
----

Let us know if you have any questions or feel free to move on to the next section!
