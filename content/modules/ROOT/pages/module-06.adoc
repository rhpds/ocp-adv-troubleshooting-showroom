= Debugging OVN-Kubernetes - The Setup
:prewrap!:

The Software Defined Network (SDN) used in the OpenShift Container Platform is based off the upstream OVN-Kubernetes project and serves as the backbone for all container and virtual machine networking needs.

OVN-Kubernetes (Open Virtual Networking - Kubernetes) is an open-source project that provides a robust networking solution for Kubernetes clusters with OVN (Open Virtual Networking) and Open vSwitch (Open Virtual Switch) at its core. It is a Kubernetes networking conformant plugin written according to the CNI (Container Network Interface) specifications.

Everyone knows it's there, but how do we debug it? When you run into an issue, how do you narrow down the cause?

In this section of the lab, we are going to walk you through 3 scenarios and how to leverage tools like `ovnkube-trace`, `ovn-trace` and `ovn-nbctl` to identify and resolve network communication issues between internal and external resouces.


[#thetools]
== Lab Tools

[#ovnkube]
=== ovnkube-trace

Th first tool you'll go to, `ovnkube-trace`, is a GO based command line tool that takes a set of arguments, inspects your running OpenShift cluster and translates all of the pod IP and MAC information to perform low level ovn-trace, ovs-appctl ofproto/trace and ovn-detrace commands on your defined workloads.

Because this lab is not taking place on a running OpenShift cluster, we will show you examples of the `ovnkube-trace` command for each exercise and actually use the lower level `ovn-trace` that `ovnkube-trace` produces.

[#ovntrace]
=== ovn-trace

Moving down a layer, `ovn-trace` is used to simulate packet forwarding within an OVN logical network. This allows you to view and subsequently debug where and why decision were made that resulted in the successful or unsuccessful delivery of your packet.

[#ovnnb]
=== ovn-nbctl

The `ovn-nbctl` command allows you to interact with the running OVN Northbound Database. In this lab, we will use it to view and inspect logical routrs and switchs, ACLs and other configurations that make up the SDN configuration.

[#thesetup]
== Lab Setup

[NOTE]
=====
This lab is not using a running OpenShift Cluster.

The following scenarios we're deployed into a running OpenShift Cluster and a backup of the OVN and OVS databases, along with flows and interfaces, were restored into this demo environment.

All commands and outputs yield the same results as if this OVN-Kubernetes dump was running in an OpenShift Cluster.
=====

[#theapplication]
=== The Demo Application

What does our example application look like?

For this lab, we will trace traffic flows between 2 frontend pods, 2 backend pods and 1 virtual machine with each set residing in their own namespace.

[.wrap,bash]
----
NAMESPACE  NAME                              READY   STATUS     RESTARTS  AGE   IP             NODE                                    NOM NODE   READINESS

backend    sitea-backend-796cf44b87-wx565    1/1     Running    0         27h   10.129.6.11    worker-5.ocpv.tamlab.rdu2.redhat.com    <none>     <none>

backend    siteb-backend-5b765679b5-6wkvx    1/1     Running    0         27h   10.129.6.16    worker-5.ocpv.tamlab.rdu2.redhat.com    <none>     <none>

external   virt-launcher-external-vm-tfbr4   1/1     Running    0         13m   10.129.7.5     worker-5.ocpv.tamlab.rdu2.redhat.com    <none>     1/1

frontend   sitea-frontend-8798d9cdc-sq7dk    1/1     Running    0         27h   10.129.6.17    worker-5.ocpv.tamlab.rdu2.redhat.com    <none>     <none>

frontend   siteb-frontend-66f9f55ddc-97jkg   1/1     Running    0         27h   10.129.6.15    worker-5.ocpv.tamlab.rdu2.redhat.com    <none>     <none>
----

Now that we know what the application looks like, what other information do we need? The output above gives us our pod IPs, but we also need MAC addresses and to check if any of our containers or VMs are using seconday networks. From the output below, we can see all of our IP and MAC information and that our external virtual machine is connected to a secondary network `default/vlan530`.

[.wrap,bash]
----
oc get pods sitea-frontend-8798d9cdc-sq7dk -o yaml | grep 'k8s.ovn.org/pod-networks'
    k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.129.6.17/23"],"mac_address":"0a:58:0a:81:06:11","gateway_ips":["10.129.6.1"],"routes":[{"dest":"10.128.0.0/14","nextHop":"10.129.6.1"},{"dest":"172.30.0.0/16","nextHop":"10.129.6.1"},{"dest":"169.254.0.5/32","nextHop":"10.129.6.1"},{"dest":"100.64.0.0/16","nextHop":"10.129.6.1"}],"ip_address":"10.129.6.17/23","gateway_ip":"10.129.6.1","role":"primary"}}'

oc get pods siteb-frontend-66f9f55ddc-97jkg -o yaml | grep 'k8s.ovn.org/pod-networks'
    k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.129.6.15/23"],"mac_address":"0a:58:0a:81:06:0f","gateway_ips":["10.129.6.1"],"routes":[{"dest":"10.128.0.0/14","nextHop":"10.129.6.1"},{"dest":"172.30.0.0/16","nextHop":"10.129.6.1"},{"dest":"169.254.0.5/32","nextHop":"10.129.6.1"},{"dest":"100.64.0.0/16","nextHop":"10.129.6.1"}],"ip_address":"10.129.6.15/23","gateway_ip":"10.129.6.1","role":"primary"}}'

oc get pods siteb-backend-5b765679b5-6wkvx -o yaml | grep 'k8s.ovn.org/pod-networks'
    k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.129.6.16/23"],"mac_address":"0a:58:0a:81:06:10","gateway_ips":["10.129.6.1"],"routes":[{"dest":"10.128.0.0/14","nextHop":"10.129.6.1"},{"dest":"172.30.0.0/16","nextHop":"10.129.6.1"},{"dest":"169.254.0.5/32","nextHop":"10.129.6.1"},{"dest":"100.64.0.0/16","nextHop":"10.129.6.1"}],"ip_address":"10.129.6.16/23","gateway_ip":"10.129.6.1","role":"primary"}}'

oc get pods sitea-backend-796cf44b87-wx565 -o yaml | grep 'k8s.ovn.org/pod-networks'
    k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.129.6.11/23"],"mac_address":"0a:58:0a:81:06:0b","gateway_ips":["10.129.6.1"],"routes":[{"dest":"10.128.0.0/14","nextHop":"10.129.6.1"},{"dest":"172.30.0.0/16","nextHop":"10.129.6.1"},{"dest":"169.254.0.5/32","nextHop":"10.129.6.1"},{"dest":"100.64.0.0/16","nextHop":"10.129.6.1"}],"ip_address":"10.129.6.11/23","gateway_ip":"10.129.6.1","role":"primary"}}'

oc get pods virt-launcher-external-vm-tfbr4 -o yaml | grep 'k8s.ovn.org/pod-networks'
    k8s.ovn.org/pod-networks: '{"default":{"ip_addresses":["10.129.7.5/23"],"mac_address":"0a:58:0a:81:07:05","gateway_ips":["10.129.6.1"],"routes":[{"dest":"10.128.0.0/14","nextHop":"10.129.6.1"},{"dest":"172.30.0.0/16","nextHop":"10.129.6.1"},{"dest":"169.254.0.5/32","nextHop":"10.129.6.1"},{"dest":"100.64.0.0/16","nextHop":"10.129.6.1"}],"ip_address":"10.129.7.5/23","gateway_ip":"10.129.6.1","role":"primary"},"default/vlan530":{"ip_addresses":null,"mac_address":"02:a1:3b:00:00:15","role":"secondary"}}'
----

You can also pull the networking information of the VM directly off the Virtual Machine Instance (vmi) object.

[.wrap,console]
----
oc get vmi -o wide
NAME          AGE   PHASE     IP             NODENAME                               READY   LIVE-MIGRATABLE   PAUSED

external-vm   21m   Running   10.6.153.247   worker-5.ocpv.tamlab.rdu2.redhat.com   True    True
----

[.wrap,yaml]
----
oc get vmi external-vm -o yaml | yq .status.interfaces
- infoSource: domain, guest-agent, multus-status
  interfaceName: eth0
  ipAddress: 10.6.153.247
  ipAddresses:
    - 10.6.153.247
    - 2620:52:9:1699:a1:3bff:fe00:15
    - fe80::a1:3bff:fe00:15
  linkState: up
  mac: 02:a1:3b:00:00:15
  name: default
  podInterfaceName: pod37a8eec1ce1
  queueCount: 1
----

We also know that our cluster has some default `NetworkPolicies` and `MultiNetworkPolicies` that get applied to every new `Namespace`.

[source,yaml]
----
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-deny
  namespace: frontend
spec:
  podSelector: {}
  policyTypes:
  - Ingress
----

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
  namespace: frontend
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress
----

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-backend-to-siteb-frontend
  namespace: frontend
spec:
  podSelector:
    matchLabels:
      deployment: siteb-frontend
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: backend
  policyTypes:
  - Ingress
----

[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  annotations:
    k8s.v1.cni.cncf.io/policy-for: default/vlan530
  name: deny-by-default
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
----

[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  annotations:
    k8s.v1.cni.cncf.io/policy-for: default/vlan530
  name: egressallow-gh
spec:
    egress:
    - to:
      - ipBlock:
          cidr: 140.82.113.0/24
      - ipBlock:
          cidr: 8.8.8.8/32
    podSelector:
      matchLabels:
        internet: "true"
    policyTypes:
    - Egress
----

[#environmentsetup]
=== Setup the OVN Environment

To begin, lets start our OVN environment by moving into the `ModuleOVN` directory and executing the start script.

[source,console,role=execute]
----
cd ~/ModuleOVN/
----

[source,console,role=execute]
----
/home/lab-user/ModuleOVN/ovs-dbg/bin/ovs-offline -w /home/lab-user/ModuleOVN/ovs-offline start
----

After executing the above command, you will see output of 4 containers starting.

[source,console]
----
Starting container ovsdb-server-ovn_nb
7c3f080cf89e038255fc2b35825ce69dc85440dfd48864b8b4876bc14c4473ee
Starting container ovsdb-server-ovn_sb
270f9af1197081df32789c541eaf54c77e5ccadfee560ad9895fd19572432d16
Starting container ovsdb-server-ovs
066bd7f2e5d4c5023c7e552a273cd318add2489d06c4e46cfe9e2340bcac0bc6
Starting container ovs-vswitchd
b0bc4411308193f003731553655ec405fa04f6d9848a213e6b6c653ffbe80b5e
----

When that is finished, confirm your 4 containers are running.

[source,console,role=execute]
----
podman ps
----

[source,console]
----
CONTAINER ID  IMAGE                         COMMAND         CREATED        STATUS        PORTS       NAMES
7c3f080cf89e  localhost/ovs-offline:latest  ovsdb-ovn_nb    5 minutes ago  Up 5 minutes              ovsdb-server-ovn_nb
270f9af11970  localhost/ovs-offline:latest  ovsdb-ovn_sb    5 minutes ago  Up 5 minutes              ovsdb-server-ovn_sb
066bd7f2e5d4  localhost/ovs-offline:latest  ovsdb-ovs       5 minutes ago  Up 5 minutes              ovsdb-server-ovs
b0bc44113081  localhost/ovs-offline:latest  vswitchd-dummy  4 minutes ago  Up 4 minutes              ovs-vswitchd
----

The last step is to move into the pre-created execution environment by sourcing the following file. This produces a number of `alias` commands to allow for easier execution of commands against the OVN environment.

[source,console,role=execute]
----
source /tmp/ovs-offline/bin/activate
----

You will now see your terminal change, pre-pending `(ovs-offline)` to your existing prompt `[lab-user@rhel9 ModuleOVN]$`.

[source,console]
----
* You can now run the following offline commands directly:
      ovs-vsctl [...]
      ovsdb-client [...]

      ovn-nbctl [...]
      ovsdb-client [...] $OVN_NB_DB

      ovn-sbctl [...]
      ovsdb-client [...] $OVN_SB_DB

* You can restore your previous environment with:
      ovs-offline-deactivate
(ovs-offline) [lab-user@rhel9 ModuleOVN]$
----

To confirm your environment is up, run the following command which will print an overview of the database contents.

[source,console,role=execute]
----
ovn-nbctl show
----

Now that we know what the demo environment looks like and you have started the OVN environment, let's get into the first exercise! If you have any issues or questions, let us know.
